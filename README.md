# ICLR 2020

![](https://github.com/juand-r/ICLR-2020/blob/master/trevor-cole-k7ZhGBzSM5w-unsplash-crop.jpg)
<sub><sup>[*Lalibela, Ethiopia*](https://unsplash.com/photos/k7ZhGBzSM5w) by [Trevor Cole](https://unsplash.com/@trevcole?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on Unsplash.</sup></sub>

### Selections from ICLR 2020 (mostly NLP-related)

- [Workshops](#workshops)
- [Socials](#socials)
- [Keynotes](#keynotes)
- [Papers](#papers)
  - [Adversarial ML and Robustness](#top-adversarial-ml-and-robustness)
  - [Neural Network Architectures](#top-neural-network-architectures)
  - [Compositionality](#top-compositionality)
  - [Emergent Language](#top-emergent-language)
  - [Explainability](#top-explainability)
  - [Graphs](#top-graphs)
  - [Graph Neural Networks](#top-graph-neural-networks)
  - [Knowledge Graphs](#top-knowledge-graphs)
  - [Learning with Less Labels](#top-learning-with-less-labels)
  - [Language Models and Transformers](#top-language-models-and-transformers)
  - [Reasoning](#top-reasoning)
  - [Reinforcement Learning](#top-reinforcement-learning)
  - [Style Transfer and Generative Models](#top-style-transfer-and-generative-models)
  - [Text Generation](#top-text-generation)
  - [Miscellaneous](#top-miscellaneous)

 ---
 ---
 
## [Workshops](#selections-from-iclr-2020-mostly-NLP-related)

- [**Causal Learning For Decision Making**](https://causalrlworkshop.github.io/) [(videos)](https://slideslive.com/iclr-2020/workshop-workshop-on-causal-learning-for-decision-making)
- [**Towards Trustworthy ML: Rethinking Security and Privacy for ML**](https://trustworthyiclr20.github.io/)  [(videos)](https://slideslive.com/iclr-2020/workshop-towards-trustworthy-ml-rethinking-security-and-privacy-for-ml) "Bring together experts from a variety of communities (ML, computer security, data privacy, fairness, & ethics) to work on promising ideas and research directions."
- [**Bridging AI and Cognitive Science (BAICS)**](https://baicsworkshop.github.io/) [(videos)](https://slideslive.com/iclr-2020/workshop-bridging-ai-and-cognitive-science-baics)
- [Fundamental Science in the era of AI](https://slideslive.com/iclr-2020/workshop-fundamental-science-in-the-era-of-ai)
- [Neural Architecture Search](https://sites.google.com/view/nas2020)  [(videos)](https://slideslive.com/iclr-2020/workshop-neural-architecture-search)
- [Integration of Deep Neural Models and Differential Equations](http://iclr2020deepdiffeq.rice.edu/) [(videos)](https://slideslive.com/iclr-2020/workshop-africanlp-unlocking-local-languages)
- [**Beyond 'tabula rasa' in reinforcement learning: agents that remember, adapt, and generalize**](http://www.betr-rl.ml/2020/) [(videos)](https://slideslive.com/iclr-2020/workshop-beyond-tabula-rasa-in-reinforcement-learning-agents-that-remember-adapt-and-generalize)
- [AI for Overcoming Global Disparities in Cancer Care](https://ai4cc.org/)  [(videos)](https://slideslive.com/iclr-2020/workshop-ai-for-overcoming-global-disparities-in-cancer-care)
- [AI for Affordable Healthcare](https://sites.google.com/view/ai4ah-iclr2020/home)  [(videos)](https://slideslive.com/iclr-2020/workshop-ai-for-affordable-healthcare)
- [AI for Earth Sciences](https://ai4earthscience.github.io/iclr-2020-workshop/) [(videos)](https://slideslive.com/iclr-2020/workshop-ai-for-earth-sciences)
- [Computer Vision for Agriculture (CV4A)](https://www.cv4gc.org/cv4a2020/) [(videos)](https://slideslive.com/iclr-2020/workshop-computer-vision-for-agriculture-cv4a)
- [Tackling Climate Change with ML](https://www.climatechange.ai/ICLR2020_workshop.html) [(videos)](https://slideslive.com/iclr-2020/workshop-tackling-climate-change-with-ml)
- ML-IRL: Machine Learning in Real Life
- [**AfricaNLP - Unlocking Local Languages**](https://africanlp-workshop.github.io/) [(videos)](https://slideslive.com/iclr-2020/workshop-africanlp-unlocking-local-languages)
- [**Practical ML for Developing Countries: learning under limited/low resource scenarios**](https://pml4dc.github.io/iclr2020/cfp.html) [(videos)](https://slideslive.com/iclr-2020/workshop-practical-ml-for-developing-countries-learning-under-limitedlow-resource-scenarios)

### Some workshop talks

- [Gopnik, Causal Learning in Children: When children are better learners than adults - or AI](https://slideslive.com/38926305/causal-learning-in-children-when-children-are-better-learners-than-adults-or-ai?ref=account-folder-46623-folders)

- [Algorithmic Recourse: from Counterfactual Explanations to Interventions](https://slideslive.com/38926325/algorithmic-recourse-from-counterfactual-explanations-to-interventions?ref=account-folder-46623-folders) [(paper)](https://causalrlworkshop.github.io/pdf/CLDM_24.pdf)

### Workshops, see later

- [Cyclic-Permutation Invariant Networks for Modeling Periodic Time Series](https://slideslive.com/38926214/cyclicpermutation-invariant-networks-for-modeling-periodic-time-series?ref=account-folder-46620-folders)

---
---

## [Socials](#selections-from-iclr-2020-mostly-NLP-related)

Lots of "social" events, both topic and demographic based:

- **Topics in Language Research**
- **Learning Representation for Cybersecurity**
- Research with ðŸ¤— Transformers
- ICLR Town:
  ![](https://github.com/juand-r/ICLR-2020/blob/master/iclrtown.png)

---
---

## [Keynotes](#selections-from-iclr-2020-mostly-NLP-related)

1. [Aisha Walcott-Bryant: AI + Africa = Global Innovation](https://iclr.cc/virtual/speaker_1.html)
2. [Leslie Kaelbling: Doing for Our Robots What Nature Did For Us](https://iclr.cc/virtual/speaker_2.html)
3. [**Ruha Benjamin: 2020 Vision: Reimagining the Default Settings of Technology & Society**](https://iclr.cc/virtual/speaker_3.html)
   - A discussion on how even apparently neutral technology can perpetuate discrimination. Technologists and researchers should be aware of the societal consequences of their work. 
4. [**Laurent Dinh: Invertible Models and Normalizing Flows**](https://iclr.cc/virtual/speaker_4.html)
5. [Mihaela van der Schaar: Machine Learning: Changing the future of healthcare](https://iclr.cc/virtual/speaker_5.html)
6. [Devi Parikh: AI Systems That Can See And Talk](https://iclr.cc/virtual/speaker_6.html)
7. [Yann LeCun and Yoshua Bengio: Reflections from the Turing Award Winners](https://iclr.cc/virtual/speaker_7.html)

   - Yann LeCunn: "The future is self-supervised". Challenges for Deep Learning: (1) Learning with **less labeled data** (self-supervised learning!), (2) how to make **reasoning** compatible with gradient-based learning, i.e., beyond 'system 1', and (3) learning complex (hierarchical) action sequences (nothing to say here). Mostly a discussion of energy-based models (not too different from [previous talks](https://www.math.ias.edu/files/special_year_workshops/lecun-20191016-ias.pdf)). "Could energy-based SSL be a basis for common sense?"
   
   - Yoshua Bengio: "Deep learning priors associated with conscious processing". Similar to this other [recent talk](https://slideslive.com/38922794/towards-compositional-understanding-of-the-world-by-agentbased-deep-learning).
     - ML and Consciousness ("Consciousness Prior")
     - The need for systematic generalization by dynamically recombining existing concepts, but avoiding the pitfalls of classical AI (e.g., need uncertaining handling, distributed representation, efficient search, grounding in 'system 1' and large-scale training).

8. [**Michael I. Jordan: The Decision-Making Side of Machine Learning: Dynamical, Statistical and Economic Perspectives**](https://iclr.cc/virtual/speaker_8.html) Note: see also [Artificial Intelligenceâ€”The Revolution Hasnâ€™t Happened Yet](https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/8)

---
---


## [Papers](#selections-from-iclr-2020-mostly-NLP-related)

**Observations:** there was less of a distinction between posters and orals than in an IRL conference, as posters were just short talks. I thought the "poster" format worked very well, but I was much less likely to interact with the authors than in a non-virtual conference.

**Some popular topics:** reinforcement learning, adversarial ML, graph neural networks.

See also:
- https://twitter.com/sjmielke/status/1256732328904347648


### [:top:](#selections-from-iclr-2020-mostly-NLP-related) Adversarial ML and Robustness

:boom: [Robustness Verification for Transformers](https://iclr.cc/virtual/poster_BJxwPJHFwS.html)   [**(paper)**](https://openreview.net/pdf?id=BJxwPJHFwS)   [**(reviews)**](https://openreview.net/forum?id=BJxwPJHFwS) [**(code)**](https://github.com/shizhouxing/Robustness-Verification-for-Transformers)
> TL;DR: "We propose the first algorithm for verifying the robustness of Transformers."


:heavy_minus_sign: [Distributionally Robust Neural Networks for Group Shifts](https://iclr.cc/virtual/poster_ryxGuJrFvS.html)  [**(paper)**](https://openreview.net/pdf?id=ryxGuJrFvS)  [**(reviews)**](https://openreview.net/forum?id=ryxGuJrFvS) [**(code)**](https://github.com/kohpangwei/group_DRO)
> **Problem:** models often "latch onto" **spurious correlations**: features that work on most training examples but don't solve the problem as we would expect.  E.g., image classification -- waterbird and water background often (but not always co-occur). Overall accuracy may be high, but worst-group accuracy (e.g., waterbirds on land) can be very low.
>
> **Goal:** achieve models that are more robust to spurious correlations with lower worst-group error.
> 
> **Solution:** Group distributionally robust optimization (DRO): minimize the worst-group's average loss, rather than the (overall) average loss. This requires knowing groups (attributes and labels) for each training example (but not at test time). A stochastic optimization algorithm is proposed and convergence guarantees are derived.
>
> **But:** the worst-group error of Group DRO (at test time) is still high, i.e.,poor generalization!  Previous work on small convex or generative models says this shouldn't happen.  This happens because the models are SOTA overparametrized neural networks. To solve this, use stronger regularization than usual (L2 penalty).
>
> **Evaluation:** Two image classification datasets (CelebA and Waterbirds) and one NLI dataset (MultiNLI).

**The remaining papers deal with adversarial ML in computer vision:**

:boom: [Unrestricted Adversarial Examples via Semantic Manipulation](https://iclr.cc/virtual/poster_Sye_OgHFwH.html) [**(paper)**](https://openreview.net/pdf?id=Sye_OgHFwH)  [**(reviews)**](https://openreview.net/forum?id=Sye_OgHFwH) [**(code)**](https://www.dropbox.com/s/69zx437t1dgo41b/semantic_attack_code.zip?dl=0)
> **Problem:** adversarial examples (for images) are often created using perturbations within a small ball; it is easy to defend against them using JPEG compression or randomized smoothing.
>
> **Contributions:** introduce "semantically motivated" adversarial perturbations (manipulating color and texture) with no l_p bounds (unlike most perturbations in the literature, these are large, structured, explainable). It is shown these fool some common defenses (JPEG 75, Feature Squeezing, and adversarially-trained models).
> - Colorization attack: use a pre-trained colorization model and "color hints" to colorize an image in order to fool the classifier. But need to do it carefully in order to keep the colors similar to the original colors.
> - Texture attack: style transfer (transfer texture from another image). This works best with an image from the target adversarial class, but with similar features to the original image.
>
> **Evaluation:**
> - Misclassification rate under various defenses. Also, attacks transfer. 
> - User study: humans have difficulty in detecting the attack.
> - **Caption attack**: these adversarial images also fool image captioning systems! E.g., "A man is holding an apple" -> "A dog is holding an apple").


:heavy_minus_sign: [Adversarial Training and Provable Defenses: Bridging the Gap](https://iclr.cc/virtual/poster_SJxSDxrKDr.html)   [**(paper)**](http://www.openreview.net/pdf?id=SJxSDxrKDr)   [**(reviews)**](http://www.openreview.net/forum?id=SJxSDxrKDr)   [**(code)**](https://github.com/eth-sri/colt)
> TL;DR: "We propose a novel combination of adversarial training and provable defenses which produces a model with state-of-the-art accuracy and certified robustness on CIFAR-10."


:heavy_minus_sign: [Fast is better than free: Revisiting adversarial training](https://iclr.cc/virtual/poster_BJx040EFvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=BJx040EFvH)   [**(reviews)**](http://www.openreview.net/forum?id=BJx040EFvH)   [**(code)**](https://github.com/anonymous-sushi-armadillo)
> TL;DR: "FGSM-based adversarial training, with randomization, works just as well as PGD-based adversarial training: we can use this to train a robust classifier in 6 minutes on CIFAR10, and 12 hours on ImageNet, on a single machine." (cheaper than PGD). 


:heavy_minus_sign: [A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning](https://iclr.cc/virtual/poster_BylVcTNtDS.html)   [**(paper)**](http://www.openreview.net/pdf?id=BylVcTNtDS)   [**(reviews)**](http://www.openreview.net/forum?id=BylVcTNtDS)   [**(code)**](https://github.com/shrezaei/Target-Agnostic-Attack)
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Neural Network Architectures

**NOTE:** [Transformers](#top-language-models-and-transformers) and [Graph Neural Networks](#top-graph-neural-networks) get their own categories.

:boom: [Neural Stored-program Memory](https://iclr.cc/virtual/poster_rkxxA24FDr.html)  [**(paper)**](https://openreview.net/pdf?id=rkxxA24FDr)   [**(reviews)**](https://openreview.net/forum?id=rkxxA24FDr)   [**(code)**](https://github.com/thaihungle/NSM)
> 
> Presents a new architecture which simulates a Universal Turing Machine.


:heavy_minus_sign: [Mogrifier LSTM](https://iclr.cc/virtual/poster_SJe5P6EYvS.html)   [**(paper)**](https://openreview.net/pdf?id=SJe5P6EYvS)  [**(reviews)**](https://openreview.net/forum?id=SJe5P6EYvS)   [**(code)**](https://github.com/deepmind/lamb) 
> Initial motivation -- input embeddings for language models are based on the *average* context; it might be better (particularly for verbs and function words) to use the *actual* context. But forget this! "Mogrify" the LSTM by adding more than one round of gating. This achieves lower perplexity than LSTMs and Transformer XL (on Penn Treebank and Wikitext-2).
>
> Why does the Mogrifier work? There are many plausible reasons, none of them fully convincing. On a synthetic dataset, the Mogrifier LSTM also outperforms the LSTM (with larger gains for larger vocabulary size). "Sadly, we could not escape the deep learning pit and a convincing explanation remained elusive".


:heavy_minus_sign: [On the Relationship between Self-Attention and Convolutional Layers](https://iclr.cc/virtual/poster_HJlnC1rKPB.html) [**(paper)**](https://openreview.net/pdf?id=HJlnC1rKPB)   [**(reviews)**](https://openreview.net/forum?id=HJlnC1rKPB)   [**(code)**](https://github.com/epfml/attention-cnn)
> TL;DR: "A self-attention layer can perform convolution and often learns to do so in practice."
>
> Transformers are great at NLP tasks. They can also reach SOTA accuracy on vision tasks (Bello et al. 2019; Ramachandran et al., 2019). Why does self-attention work so well for images? This paper shows that multi-head self-attention can express convolutions.
>
> **Blog:** http://jbcordonnier.com/posts/attention-cnn/
>
> **Demo:** https://epfml.github.io/attention-cnn/


---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Compositionality


:boom: [Permutation Equivariant Models for Compositional Generalization in Language](https://iclr.cc/virtual/poster_SylVNerFvr.html)   [**(paper)**](https://openreview.net/pdf?id=SylVNerFvr)   [**(reviews)**](https://openreview.net/forum?id=SylVNerFvr)   [**(code)**](https://github.com/facebookresearch/Permutation-Equivariant-Seq2Seq)
> TL;DR: "We propose a link between permutation equivariance and compositional generalization, and provide equivariant language models."
>
> Compositionality example: if one understands "Today I will **run twice**" and "I **walk** to school every day", one should also understand "I will have to **walk twice** around the store". The SCAN benchmark: machine translation between simple natural language commands (e.g., "jump", "walk left", "turn right twice") and 'machine actions' (e.g., JUMP, LTURN WALK, RTURN RTURN RTURN).


:boom: [Measuring Compositional Generalization: A Comprehensive Method on Realistic Data
](https://iclr.cc/virtual/poster_SygcCnNKwr.html)   [**(paper)**](http://www.openreview.net/pdf?id=SygcCnNKwr)   [**(reviews)**](http://www.openreview.net/forum?id=SygcCnNKwr)   [**(code)**](https://github.com/google-research/google-research/tree/master/cfq)
> TL;DR: "Benchmark and method to measure compositional generalization by maximizing divergence of compound frequency at small divergence of atom frequency."
>
> **Compositional Generalization:** ability to generalize to unseen combinations of known components (atoms).
>
> **Goal:** want to measure how much compositional generalization is required for a given train/test split.
>
> **"Compound divergence"**: a more comprehensive measure than previous approaches, assuming that (1) all test atoms occur in training, (2) the distribution of atoms is similar in train and test and (3) distribution of compounds is different between train and test.  Compound divergence correlates well with previous ad-hoc methods.
>
> **Evaluation:** Compositional Freebase Questions (CFQ) and SCAN.  An LSTM+attention, Transformer and Universal Transformer are compared. Compound Divergence is a great predictor of accuracy! Current systems fail to generalize compositionally, even with large training data, while random split is easy. (But it appears Transformers outperform LSTM+attention by a wide margin for almost every value of compound divergence).


:heavy_minus_sign: [Environmental drivers of systematicity and generalization in a situated agent](https://iclr.cc/virtual/poster_SklGryBtwr.html)   [**(paper)**](https://openreview.net/pdf?id=SklGryBtwr)   [**(reviews)**](https://openreview.net/forum?id=SklGryBtwr) 
> TL;DR: "We isolate the environmental and training factors that contribute to emergent systematic generalization in a situated language-learning agent."
>
> **From the discussion:** see [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247).


:heavy_minus_sign: [Compositional Language Continual Learning](https://iclr.cc/virtual/poster_rklnDgHtDS.html)   [**(paper)**](http://www.openreview.net/pdf?id=rklnDgHtDS)   [**(reviews)**](http://www.openreview.net/forum?id=rklnDgHtDS)   [**(code)**](https://github.com/yli1/CLCL)
> Goal: continually learn new words in seq2seq tasks (e.g., instruction learning using the SCAN dataset and machine translation).


---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Emergent Language


:heavy_minus_sign: [Compositional languages emerge in a neural iterated learning model](https://iclr.cc/virtual/poster_HkePNpVKPB.html)   [**(paper)**](https://openreview.net/pdf?id=HkePNpVKPB)   [**(reviews)**](https://openreview.net/forum?id=HkePNpVKPB)   [**(code)**](https://github.com/Joshua-Ren/Neural_Iterated_Learning)
> TL;DR: "Use iterated learning framework to facilitate the dominance of high compositional language in multi-agent games."


:heavy_minus_sign: [On the interaction between supervision and self-play in emergent communication](https://iclr.cc/virtual/poster_rJxGLlBtwH.html)   [**(paper)**](https://openreview.net/pdf?id=rJxGLlBtwH)   [**(reviews)**](http://www.openreview.net/forum?id=rJxGLlBtwH)   [**(code)**](https://github.com/backpropper/s2p)
> 


---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Explainability


:boom: [Learning The Difference That Makes A Difference With Counterfactually-Augmented Data](https://iclr.cc/virtual/poster_Sklgs0NFvr.html) [**(reviews)**](https://openreview.net/forum?id=Sklgs0NFvr)  [**(paper)**](https://openreview.net/pdf?id=Sklgs0NFvr)   [**(code)**]( https://github.com/acmi-lab/counterfactually-augmented-data)
> TL;DR: "Humans in the loop revise documents to accord with counterfactual labels, resulting resource helps to reduce reliance on spurious associations."
>
> **See also:** [Evaluating NLP Models via Contrast Sets](https://arxiv.org/pdf/2004.02709.pdf)


:boom: [Explanation by Progressive Exaggeration](https://iclr.cc/virtual/poster_H1xFWgrFPS.html)  [**(paper)**](https://openreview.net/pdf?id=H1xFWgrFPS)  [**(reviews)**]( https://openreview.net/forum?id=H1xFWgrFPS)   [**(code)**]( https://github.com/batmanlab/Explanation_by_Progressive_Exaggeration)
> TL;DR: "A method to explain a classifier, by generating visual perturbation of an image by exaggerating  or diminishing the semantic features that the classifier associates with a target label."
>
> Creating image counterfactuals with GANs.


:heavy_minus_sign: [N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://iclr.cc/virtual/poster_r1ecqn4YwB.html) [**(paper)**](https://openreview.net/pdf?id=r1ecqn4YwB)  [**(reviews)**](https://openreview.net/forum?id=r1ecqn4YwB)
> TL;DR: "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets."


:heavy_minus_sign: [Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models](https://iclr.cc/virtual/poster_BkxRRkSKwr.html)  [**(paper)**](https://openreview.net/pdf?id=BkxRRkSKwr)    [**(reviews)**](https://openreview.net/forum?id=BkxRRkSKwr) [**(code)**](https://github.com/ink-usc/hierarchical-explanation) [**(project)**](https://inklab.usc.edu/hiexpl/)
> TL;DR: "We propose measurement of phrase importance and algorithms for hierarchical explanation of neural sequence model predictions."


---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Graphs

:boom: [Spectral Embedding of Regularized Block Models](https://iclr.cc/virtual/poster_H1l_0JBYwS.html)   [**(paper)**](http://www.openreview.net/pdf?id=H1l_0JBYwS)   [**(reviews)**](http://www.openreview.net/forum?id=H1l_0JBYwS)   [**(code)**](https://github.com/nathandelara/Spectral-Embedding-of-Regularized-Block-Models/)
> 

:heavy_minus_sign: [GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding](https://iclr.cc/virtual/poster_r1lGO0EKDH.html)   [**(paper)**](https://openreview.net/pdf?id=r1lGO0EKDH)   [**(reviews)**](http://www.openreview.net/forum?id=r1lGO0EKDH)   [**(code)**](https://github.com/cornell-zhang/GraphZoom)
> 


:heavy_minus_sign: [Low-dimensional statistical manifold embedding of directed graphs](https://iclr.cc/virtual/poster_SkxQp1StDH.html)   [**(paper)**](http://www.openreview.net/pdf?id=SkxQp1StDH)   [**(reviews)**](http://www.openreview.net/forum?id=SkxQp1StDH)
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Graph Neural Networks

There were a lot of papers about GNNs.  Here is a few I found interesting:

:heavy_minus_sign: [A Fair Comparison of Graph Neural Networks for Graph Classification](https://iclr.cc/virtual/poster_HygDF6NFPB.html)   [**(paper)**](http://www.openreview.net/pdf?id=HygDF6NFPB)   [**(reviews)**](http://www.openreview.net/forum?id=HygDF6NFPB)   [**(code)**](https://github.com/diningphil/gnn-comparison)
>

:heavy_minus_sign: [On the Equivalence between Positional Node Embeddings and Structural Graph Representations](https://iclr.cc/virtual/poster_SJxzFySKwH.html)   [**(paper)**](http://www.openreview.net/pdf?id=SJxzFySKwH)   [**(reviews)**](http://www.openreview.net/forum?id=SJxzFySKwH)   [**(code)**](https://github.com/PurdueMINDS/Equivalence)
> 

:heavy_minus_sign: [LambdaNet: Probabilistic Type Inference using Graph Neural Networks](https://iclr.cc/virtual/poster_Hkx6hANtwH.html)   [**(paper)**](http://www.openreview.net/pdf?id=Hkx6hANtwH)   [**(reviews)**](http://www.openreview.net/forum?id=Hkx6hANtwH)   [**(code)**](https://github.com/MrVPlusOne/LambdaNet)
> 

:heavy_minus_sign: [Strategies for Pre-training Graph Neural Networks](https://iclr.cc/virtual/poster_HJlWWJSFDH.html)   [**(paper)**](http://www.openreview.net/pdf?id=HJlWWJSFDH)   [**(reviews)**](http://www.openreview.net/forum?id=HJlWWJSFDH)   [**(code)**](https://github.com/snap-stanford/pretrain-gnns/)
> 

:heavy_minus_sign: [What graph neural networks cannot learn: depth vs width](https://iclr.cc/virtual/poster_B1l2bp4YwS.html)   [**(paper)**](http://www.openreview.net/pdf?id=B1l2bp4YwS)   [**(reviews)**](http://www.openreview.net/forum?id=B1l2bp4YwS)
> 

:heavy_minus_sign: [The Logical Expressiveness of Graph Neural Networks](https://iclr.cc/virtual/poster_r1lZ7AEKvB.html)   [**(paper)**](http://www.openreview.net/pdf?id=r1lZ7AEKvB)   [**(reviews)**](http://www.openreview.net/forum?id=r1lZ7AEKvB)   [**(code)**](https://github.com/juanpablos/GNN-logic)
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Knowledge Graphs

:heavy_minus_sign: [Probability Calibration for Knowledge Graph Embedding Models](https://iclr.cc/virtual/poster_S1g8K1BFwS.html)   [**(paper)**](http://www.openreview.net/pdf?id=S1g8K1BFwS)   [**(reviews)**](http://www.openreview.net/forum?id=S1g8K1BFwS)   [**(code)**](https://github.com/Accenture/AmpliGraph/)
> 

:heavy_minus_sign: [Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings](https://iclr.cc/virtual/poster_BJgr4kSFDS.html)   [**(paper)**](http://www.openreview.net/pdf?id=BJgr4kSFDS)   [**(reviews)**](http://www.openreview.net/forum?id=BJgr4kSFDS)   [**(code)**](https://github.com/hyren/query2box) [**(project)**](http://snap.stanford.edu/query2box/)
> 

:heavy_minus_sign: [You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings](https://iclr.cc/virtual/poster_BkxSmlBFvr.html)   [**(paper)**](https://openreview.net/pdf?id=BkxSmlBFvr)   [**(reviews)**](https://openreview.net/forum?id=BkxSmlBFvr)   [**(code)**](https://github.com/uma-pi1/kge)
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Learning with Less Labels


:boom: [Few-shot Text Classification with Distributional Signatures](https://iclr.cc/virtual/poster_H1emfT4twB.html)   [**(paper)**](https://openreview.net/pdf?id=H1emfT4twB)   [**(reviews)**](https://openreview.net/forum?id=H1emfT4twB)   [**(code)**](https://github.com/YujiaBao/Distributional-Signatures)
> 


:boom: [Learning from Rules Generalizing Labeled Exemplars](https://iclr.cc/virtual/poster_SkeuexBtDr.html)   [**(paper)**](http://www.openreview.net/pdf?id=SkeuexBtDr)   [**(reviews)**](http://www.openreview.net/forum?id=SkeuexBtDr)   [**(code)**](https://github.com/awasthiabhijeet/Learning-From-Rules)
> 


:heavy_minus_sign: [Locality and Compositionality in Zero-Shot Learning](https://iclr.cc/virtual/poster_Hye_V0NKwr.html)   [**(paper)**](http://www.openreview.net/pdf?id=Hye_V0NKwr)   [**(reviews)**](http://www.openreview.net/forum?id=Hye_V0NKwr)
> 

:heavy_minus_sign: [Graph inference learning for semi-supervised classification](https://iclr.cc/virtual/poster_r1evOhEKvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=r1evOhEKvH)   [**(reviews)**](http://www.openreview.net/forum?id=r1evOhEKvH) 
> 

:heavy_minus_sign: [Automatically Discovering and Learning New Visual Categories with Ranking Statistics](https://iclr.cc/virtual/poster_BJl2_nVFPB.html)   [**(paper)**](http://www.openreview.net/pdf?id=BJl2_nVFPB)   [**(reviews)**](http://www.openreview.net/forum?id=BJl2_nVFPB)   [**(code)**](http://www.robots.ox.ac.uk/~vgg/research/auto_novel/)
> 


---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Language Models and Transformers

**NOTE:** [here](https://towardsdatascience.com/whats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792) is another summary of some of the papers on Transformers.

:boom: [Generalization through Memorization: Nearest Neighbor Language Models](https://iclr.cc/virtual/poster_HklBjCEKvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=HklBjCEKvH)   [**(reviews)**](http://www.openreview.net/forum?id=HklBjCEKvH)   [**(code)**](https://github.com/urvashik/knnlm)
> 
> **From the discussion:** see also "Forgetting Exceptions is Harmful in Language Learning" (1998) https://arxiv.org/abs/cs/9812021, on the same theme of generalization vs memorization.


:boom: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://iclr.cc/virtual/poster_H1eA7AEtvS.html)   [**(paper)**](http://www.openreview.net/pdf?id=H1eA7AEtvS)   [**(reviews)**](http://www.openreview.net/forum?id=H1eA7AEtvS)   [**(code)**](https://github.com/google-research/ALBERT)
> 

:boom: [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://iclr.cc/virtual/poster_r1xMH1BtvB.html)   [**(paper)**](http://www.openreview.net/pdf?id=r1xMH1BtvB)   [**(reviews)**](http://www.openreview.net/forum?id=r1xMH1BtvB)   [**(code)**](https://github.com/google-research/electra)
> 

:boom: [Reformer: The Efficient Transformer](https://iclr.cc/virtual/poster_rkgNKkHtvB.html)   [**(paper)**](http://www.openreview.net/pdf?id=rkgNKkHtvB)   [**(reviews)**](http://www.openreview.net/forum?id=rkgNKkHtvB)   [**(code)**](https://github.com/google/trax/tree/master/trax/models/reformer)
> 

:heavy_minus_sign: [Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention](https://iclr.cc/virtual/poster_r1eIiCNYwS.html)   [**(paper)**](http://www.openreview.net/pdf?id=r1eIiCNYwS)   [**(reviews)**](http://www.openreview.net/forum?id=r1eIiCNYwS)   [**(code)**](https://github.com/microsoft/Transformer-XH)
> 

:heavy_minus_sign: [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://iclr.cc/virtual/poster_BJgQ4lSFPH.html)   [**(paper)**](http://www.openreview.net/pdf?id=BJgQ4lSFPH)   [**(reviews)**](http://www.openreview.net/forum?id=BJgQ4lSFPH)
> 
> **From the discussion:**

:heavy_minus_sign: [Lite Transformer with Long-Short Range Attention](https://iclr.cc/virtual/poster_ByeMPlHKPH.html)   [**(paper)**](http://www.openreview.net/pdf?id=ByeMPlHKPH)   [**(reviews)**](http://www.openreview.net/forum?id=ByeMPlHKPH)   [**(code)**](https://github.com/mit-han-lab/lite-transformer)
> 

:heavy_minus_sign: [Compressive Transformers for Long-Range Sequence Modelling](https://iclr.cc/virtual/poster_SylKikSYDH.html)   [**(paper)**](http://www.openreview.net/pdf?id=SylKikSYDH)   [**(reviews)**](http://www.openreview.net/forum?id=SylKikSYDH)
> 

:heavy_minus_sign: [Depth-Adaptive Transformer](https://iclr.cc/virtual/poster_SJg7KhVKPH.html)   [**(paper)**](http://www.openreview.net/pdf?id=SJg7KhVKPH)   [**(reviews)**](http://www.openreview.net/forum?id=SJg7KhVKPH)
> 

:heavy_minus_sign: [LAMOL: LAnguage MOdeling for Lifelong Language Learning](https://iclr.cc/virtual/poster_Skgxcn4YDS.html)   [**(paper)**](http://www.openreview.net/pdf?id=Skgxcn4YDS)   [**(reviews)**](http://www.openreview.net/forum?id=Skgxcn4YDS)   [**(code)**](https://github.com/jojotenya/LAMOL)
> 

:heavy_minus_sign: [Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models](https://iclr.cc/virtual/poster_HkgaETNtDB.html)   [**(paper)**](https://openreview.net/pdf?id=HkgaETNtDB)   [**(reviews)**](http://www.openreview.net/forum?id=HkgaETNtDB)   [**(code)**](https://github.com/bloodwass/mixout)
> 

:heavy_minus_sign: [On Identifiability in Transformers](https://iclr.cc/virtual/poster_BJg1f6EFDB.html)   [**(paper)**](http://www.openreview.net/pdf?id=BJg1f6EFDB)   [**(reviews)**](http://www.openreview.net/forum?id=BJg1f6EFDB) 
> 
> **From the discussion:**

:heavy_minus_sign: [Are Transformers universal approximators of sequence-to-sequence functions?](https://iclr.cc/virtual/poster_ByxRM0Ntvr.html)   [**(paper)**](http://www.openreview.net/pdf?id=ByxRM0Ntvr)   [**(reviews)**](http://www.openreview.net/forum?id=ByxRM0Ntvr) 
> 

:heavy_minus_sign: [Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction](https://iclr.cc/virtual/poster_H1xPR3NtPB.html)   [**(paper)**](http://www.openreview.net/pdf?id=H1xPR3NtPB)   [**(reviews)**](http://www.openreview.net/forum?id=H1xPR3NtPB)   [**(code)**](https://github.com/galsang/trees_from_transformers)
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Reasoning


:heavy_minus_sign: [Neural Module Networks for Reasoning over Text](https://iclr.cc/virtual/poster_SygWvAVFPr.html)   [**(paper)**](http://www.openreview.net/pdf?id=SygWvAVFPr)   [**(reviews)**](http://www.openreview.net/forum?id=SygWvAVFPr)   [**(code)**](https://nitishgupta.github.io/nmn-drop/)
> 

:heavy_minus_sign: [Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension](https://iclr.cc/virtual/poster_ryxjnREFwH.html)   [**(paper)**](http://www.openreview.net/pdf?id=ryxjnREFwH)   [**(reviews)**](http://www.openreview.net/forum?id=ryxjnREFwH)
> 

:heavy_minus_sign: [ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning](https://iclr.cc/virtual/poster_HJgJtT4tvB.html)   [**(paper)**](http://www.openreview.net/pdf?id=HJgJtT4tvB)   [**(reviews)**](http://www.openreview.net/forum?id=HJgJtT4tvB)   [**(code)**](http://whyu.me/reclor/)
> 

:heavy_minus_sign: []()   [**(paper)**]()   [**(reviews)**]()   [**(code)**]()
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Reinforcement Learning

:heavy_minus_sign: [Making Efficient Use of Demonstrations to Solve Hard Exploration Problems](https://iclr.cc/virtual/poster_SygKyeHKDH.html)   [**(paper)**](http://www.openreview.net/pdf?id=SygKyeHKDH)   [**(reviews)**](http://www.openreview.net/forum?id=SygKyeHKDH)   [**(code "coming soon")**]
> 

:heavy_minus_sign: [SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards](https://iclr.cc/virtual/poster_S1xKd24twB.html)   [**(paper)**](http://www.openreview.net/pdf?id=S1xKd24twB)   [**(reviews)**](http://www.openreview.net/forum?id=S1xKd24twB)
> 

:heavy_minus_sign: [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://iclr.cc/virtual/poster_H1eCw3EKvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=H1eCw3EKvH)   [**(reviews)**](http://www.openreview.net/forum?id=H1eCw3EKvH)
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Style Transfer and Generative Models

:boom: [A Probabilistic Formulation of Unsupervised Text Style Transfer](https://iclr.cc/virtual/poster_HJlA0C4tPS.html)   [**(paper)**](https://openreview.net/pdf?id=HJlA0C4tPS)   [**(reviews)**](https://openreview.net/forum?id=HJlA0C4tPS)   [**(code)**](https://github.com/cindyxinyiwang/deep-latent-sequence-model)
> TL;DR: "We formulate a probabilistic latent sequence model to tackle unsupervised text style transfer, and show its effectiveness across a suite of unsupervised text style transfer tasks."
>
> x

:heavy_minus_sign: [Adjustable Real-time Style Transfer](https://iclr.cc/virtual/poster_HJe_Z04Yvr.html)   [**(paper)**](http://www.openreview.net/pdf?id=HJe_Z04Yvr)   [**(reviews)**](http://www.openreview.net/forum?id=HJe_Z04Yvr)   [**(code)**](https://sites.google.com/view/stochss/)
>  TL;DR: "Stochastic style transfer with adjustable features."

:heavy_minus_sign: [Controlling generative models with continuous factors of variations](https://iclr.cc/virtual/poster_H1laeJrKDB.html)   [**(paper)**](https://openreview.net/pdf?id=H1laeJrKDB)   [**(reviews)**](http://www.openreview.net/forum?id=H1laeJrKDB)
> TL;DR: "A model to control the generation of images with GAN and beta-VAE with regard to scale and position of the objects."
>
> x

:heavy_minus_sign: [Understanding the Limitations of Conditional Generative Models](https://iclr.cc/virtual/poster_r1lPleBFvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=r1lPleBFvH)   [**(reviews)**](http://www.openreview.net/forum?id=r1lPleBFvH) 
> 

---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Text Generation


:boom: [Plug and Play Language Models: A Simple Approach to Controlled Text Generation](https://iclr.cc/virtual/poster_H1edEyBKDS.html)   [**(paper)**](http://www.openreview.net/pdf?id=H1edEyBKDS)   [**(reviews)**](http://www.openreview.net/forum?id=H1edEyBKDS)   [**(code)**](https://github.com/uber-research/PPLM)
>
> **Blog:** https://eng.uber.com/pplm/


:heavy_minus_sign: [Decoding As Dynamic Programming For Recurrent Autoregressive Models](https://iclr.cc/virtual/poster_HklOo0VFDH.html)   [**(paper)**](http://www.openreview.net/pdf?id=HklOo0VFDH)   [**(reviews)**](http://www.openreview.net/forum?id=HklOo0VFDH)  [**(code)**](https://github.com/Najamxaidi/Decoding-as-a-dynamic-program-for-recurrent-autoregressive-models)
> 
> **Evaluation:** Text infilling task (on SWAG and Daily Dialogue datasets); this method outperforms unidirectional decoding baselines.

:heavy_minus_sign: [BERTScore: Evaluating Text Generation with BERT](https://iclr.cc/virtual/poster_SkeHuCVFDr.html)   [**(paper)**](http://www.openreview.net/pdf?id=SkeHuCVFDr)   [**(reviews)**](http://www.openreview.net/forum?id=SkeHuCVFDr)   [**(code)**](https://github.com/Tiiiger/bert_score)
> 
>
> **From the discussion:**


:heavy_minus_sign: [The Curious Case of Neural Text Degeneration](https://iclr.cc/virtual/poster_rygGQyrFvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=rygGQyrFvH)   [**(reviews)**](http://www.openreview.net/forum?id=rygGQyrFvH)   [**(code)**](https://github.com/ari-holtzman/degen)
>
> The "nucleus sampling" (top-p sampling) paper.


:heavy_minus_sign: [Neural Text Generation With Unlikelihood Training](https://iclr.cc/virtual/poster_SJeYe0NtvH.html)   [**(paper)**](http://www.openreview.net/pdf?id=SJeYe0NtvH)   [**(reviews)**](http://www.openreview.net/forum?id=SJeYe0NtvH)   [**(code)**](https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj)
>


:heavy_minus_sign: [Data-dependent Gaussian Prior Objective for Language Generation](https://iclr.cc/virtual/poster_S1efxTVYDr.html)   [**(paper)**](http://www.openreview.net/pdf?id=S1efxTVYDr)   [**(reviews)**](http://www.openreview.net/forum?id=S1efxTVYDr)   [**(code)**](https://drive.google.com/file/d/1q8PqhF9eOLOHOcOCGVKXtA_OlP6qq2mn)
>


:heavy_minus_sign: [Self-Adversarial Learning with Comparative Discrimination for Text Generation](https://iclr.cc/virtual/poster_B1l8L6EtDS.html)   [**(paper)**](http://www.openreview.net/pdf?id=B1l8L6EtDS)   [**(reviews)**](http://www.openreview.net/forum?id=B1l8L6EtDS)
>


:heavy_minus_sign: [Residual Energy-Based Models for Text Generation](https://iclr.cc/virtual/poster_B1l4SgHKDH.html)   [**(paper)**](http://www.openreview.net/pdf?id=B1l4SgHKDH)   [**(reviews)**](http://www.openreview.net/forum?id=B1l4SgHKDH)
>

:heavy_minus_sign: [Language GANs Falling Short](https://iclr.cc/virtual/poster_BJgza6VtPB.html)   [**(paper)**](http://www.openreview.net/pdf?id=BJgza6VtPB)   [**(reviews)**](http://www.openreview.net/forum?id=BJgza6VtPB)   [**(code)**](https://github.com/pclucas14/GansFallingShort)
>


---
###  [:top:](#selections-from-iclr-2020-mostly-NLP-related) Miscellaneous

:boom: [Learning to Represent Programs with Property Signatures](https://iclr.cc/virtual/poster_rylHspEKPr.html)   [**(paper)**](https://openreview.net/pdf?id=rylHspEKPr)   [**(reviews)**](https://openreview.net/forum?id=rylHspEKPr)   [**(code)**](https://github.com/brain-research/searcho)
> TL;DR: "We represent a computer program using a set of simpler programs and use this representation to improve program synthesis techniques."
>
>

:heavy_minus_sign: [Your classifier is secretly an energy based model and you should treat it like one](https://iclr.cc/virtual/poster_Hkxzx0NtDB.html)   [**(paper)**](http://www.openreview.net/pdf?id=Hkxzx0NtDB)   [**(reviews)**](http://www.openreview.net/forum?id=Hkxzx0NtDB)   [**(code)**](https://wgrathwohl.github.io/JEM/)
> TL;DR: "We show that there is a hidden generative model inside of every classifier. We demonstrate how to train this model and show the many benefits of doing so."
>
> x

:heavy_minus_sign: [A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://iclr.cc/virtual/poster_ryxWIgBFPS.html)   [**(paper)**](https://openreview.net/pdf?id=ryxWIgBFPS)   [**(reviews)**](https://openreview.net/forum?id=ryxWIgBFPS)   [**(code)**](https://github.com/ec6dde01667145e58de60f864e05a4/CausalOptimizationAnon)
> TL;DR: "This paper proposes a meta-learning objective based on speed of adaptation to transfer distributions to discover a modular decomposition and causal variables."
> 
> x

:heavy_minus_sign: [CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning](https://iclr.cc/virtual/poster_HJgzt2VKPB.html)   [**(paper)**](http://www.openreview.net/pdf?id=HJgzt2VKPB)   [**(reviews)**](http://www.openreview.net/forum?id=HJgzt2VKPB)   [**(code)**](http://rohitgirdhar.github.io/CATER)
> Have we "almost solved video understanding"? 3D convolutional models (which take time into account) only perform slightly better than their 2D counterparts.  But the temporal aspect of frames is essential. Real world video understanding requires reasoning about object permanence, estimating intentions, causal reasoning.
>
> This paper presents a new dataset, CATER (Compositional Actions and Temporal Reasoning) and a series of benchmark tasks on the dataset which requires temporal reasoning to solve. E.g., predict "rorate(cube) after slide(cone)" from the video clip. SOTA models struggle against temporal reasoning.

